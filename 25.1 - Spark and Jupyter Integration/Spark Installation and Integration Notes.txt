# NOTES for the project of Spark Installation and Jupyter Notebook Integration
Refer to these quick notes for anything related to SPARK - tips
This is broken down into 6 key parts

## Introduction
1) The Spark cluster computing framework can be combnied with basic PySpark methods to make powerful computations with big data. We can use spark in two modes:
	a) Local mode - The entire Spark application runs on a single machine. Local mode is what you'll use to prototype Spark code on your own computer. It's also easier to set up.
	b) Cluster mode - The Spark application runs across multiple machines. Cluster mode is what you'll use when you want to run your Spark application across multiple machines in a cloud environment like Amazon Web Services, Microsoft Azure, or Digital Ocean.

For now, these notes will focus on a) Local mode. Cluster mode will be focused on a future note page dealing with data engineering. 

Here is an example image of what the high-level process will look like once it is set up: https://dq-content.s3.amazonaws.com/xgRnU89.png

## Java
2) Spark runs on the Java Virtual Machine, or JVM for short, which comes in the Java SE Development Kit (JDK for short). It is recommended that you installing Java SE Development Kit version 7 or higher, which you can download from Oracle’s website: http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html

As of this writing, Java SE Development Kit 8u111 and 8u112 are the two latest releases of the JDK. Any version after JDK 7 works, so you can download any of the versions on this page. Select the appropriate installation file for your operating system. If you're on Windows or Linux, be sure to choose the correct instruction set architecture (x86 or x64) for your computer. Each computer chip has a specific instruction set architecture that determines the maximum amount of memory it can work with. The two main types are x86 (32 bit) and x64 (64-bit).

To verify that the installation worked, launch your command line application (Command Prompt for Windows and Terminal for Mac and Linux) and run: "java -version". The output should be similar to this:


java version "1.7.0_79"
Java(TM) SE Runtime Environment (build 1.7.0_79-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)


While the exact numbers probably won't match, the key thing to verify is that the version is larger than 1.7. This number actually represents Version 7. (Note: If running java -version returned an error or a different version than the one you just installed, your Java JDK installation most likely wasn't added to your PATH properly.)

## Spark
3) Because you've installed JDK, you could technically download the original source code and build Spark on your computer. Building from the source code is the process of generating an executable program for your machine. It involves many steps. While there are some performance benefits to building Spark from source, it takes a while to do, and it's hard to debug if the build fails.

To get around this, it is best to work with a pre-built version of Spark instead. Navigate to the Spark downloads page and select the following options: http://spark.apache.org/downloads.html
	a) 1.6.2 (Note: Any Spark version prior to 2.0.0 is incompatible with Python 3.6. If you have Python 3.6, it is recommended downloading one of the newer versions of Spark.)
	b) Pre-built for Hadoop 2.6
	c) Direct Download

Next, click the link that appears in Step 4 to download Spark as a .TGZ file to your computer. Open your command line application and navigate to the folder you downloaded it to. Unzip the file and move the resulting folder into your home directory. Once you have unzipped the file, move the resulting folder into your home directory.

If you have nothing to unzip the folder with (usually Windows), 7-zip is recommended for Windows: https://www.7-zip.org/

## PySpark Shell
4) PySpark is a Python library that allows us to interact with Spark objects. The source code for the PySpark library is located in the python/pyspark directory, but the executable version of the library is located in bin/pyspark. To test whether your installation built Spark properly, run the command bin/pyspark to start up the PySpark shell. The output should be similar to this picture (output is pretty long): https://dq-content.s3.amazonaws.com/vgMMYkC.png

While the output is verbose, you can see that the shell automatically initialized the SparkContext object and assigned it to the variable sc.

You don't have to run bin/pyspark from the folder that contains it. Because it's in your home directory, you can use ~/spark-1.6.1-bin-hadoop2.6/bin/pyspark to launch the PySpark shell from other directories on your machine (Note: replace 1.6.1 with 1.6.2 for newer version users). This way, you can switch to the directory that contains the data you want to use, launch the PySpark shell, and read the data in without having to use its full path. The folder you're in when you launch the PySpark shell will be the local context for working with files in Spark.

## Jupyter Notebook
5) You can make your Jupyter Notebook application aware of Spark in a few different ways. One is to create a configuration file and launch Jupyter Notebook with that configuration. Another is to import PySpark at runtime. These noets will focus on the latter approach, so you won't have to restart Jupyter Notebook each time you want to use Spark. 

First, you'll need to copy the full path to the pre-built Spark folder and set it as a shell environment variable. This way, you can specify Spark's location a single time, and every Python program you write will have access to it. If you move the Spark folder, you can change the path specification once and your code will work just fine.

For Mac/Linux: 
	a) Use nano or another text editor to open your shell environment's configuration file. If you're using the default Terminal application, the file should be in ~/.bash_profile . If you're using ZSH instead, your configuration file will be in ~/.zshrc.
	b) Add the following line to the end of the file, replacing {full path to Spark} with the actual path to Spark: export SPARK_HOME="{full path to Spark, eg /users/home/jeff/spark-2.0.1-bin-hadoop2.7/}"
	c) Exit the text editor and run either source ~/.bash_profile or source ~/.zshrc so the shell reads in and applies the update you made.

For Windows:
	a) If you've never added environment variables, read this tutorial before you proceed: https://www.pythoncentral.io/add-python-to-path-python-is-not-recognized-as-an-internal-or-external-command/
	b) https://www.pythoncentral.io/add-python-to-path-python-is-not-recognized-as-an-internal-or-external-command/

Next, simply install the findspark Python library, which looks up the location of PySpark using the environment variable we just set, using pip: "pip install findspark"

This is it for all the tools! Now to test the installation.

## Test and Conclusion
6) Download recent-grads.csv to your computer and use the command line to navigate to its location. Start Jupyter Notebook, create a new notebook, and run the following code to test your installation: https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/recent-grads.csv


# Find path to PySpark.
import findspark
findspark.init()

# Import PySpark and initialize SparkContext object.
import pyspark
sc = pyspark.SparkContext()

# Read `recent-grads.csv` in to an RDD.
f = sc.textFile('recent-grads.csv')
data = f.map(lambda line: line.split('\n'))


If you don't get any errors and can see the first 10 lines of recent-grads.csv, then you're good to go! You can use Google, StackOverflow, or the members-only Slack community to get help if you need it. Spark and big data are hard topics to understand, but you're well on your way, congrats!


